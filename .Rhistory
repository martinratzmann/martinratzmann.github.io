1087*100/13760
1404*100/13760
3345*100/13760
7.899709/10.20349
# Benötigte Pakete installieren und laden
# install.packages("udpipe", dependencies = TRUE)
# install.packages("plyr")
library(udpipe)
library(plyr)
# Englisches Modell für NLP laden (falls nötig herunterladen)
model <- udpipe_download_model(language = "english")
ud_model <- udpipe_load_model(model$file_model)
# Verzeichnis mit englischen Textdateien angeben
input_dir <- "~/Bookdown/EJIS mit Pragmatismus/data/kickstarter/fail/"  # Pfad anpassen
output_file <- "failed_kick_all_upos_counts.csv"
# Alle .txt-Dateien im Verzeichnis einlesen
text_files <- list.files(input_dir, pattern = "\\.txt$", full.names = TRUE)
# Leere Ergebnisliste vorbereiten
all_results <- list()
# Analyse für jede Datei
for (file in text_files) {
# Datei einlesen
text <- paste(readLines(file, encoding = "UTF-8", warn = FALSE), collapse = " ")
text <- trimws(text)  # führende/trailende Leerzeichen entfernen
# Nur wenn Text nicht leer ist
if (nchar(text) > 0) {
# Annotation durchführen
annotation <- udpipe_annotate(ud_model, x = text)
annotation <- as.data.frame(annotation)
# UPOS-Zählung
upos_counts <- table(annotation$upos)
upos_df <- as.data.frame(t(upos_counts))  # Transponieren für eine Zeile pro Datei
upos_df$File <- basename(file)
upos_df$Tokens <- nrow(annotation)  # Gesamtanzahl der Token
all_results[[file]] <- upos_df
}
}
# Alle Ergebnisse zu einem DataFrame zusammenführen
final_results <- do.call(rbind.fill, lapply(all_results, function(x) {
# Fehlende Spalten auffüllen
missing_cols <- setdiff(unique(unlist(lapply(all_results, names))), names(x))
for (col in missing_cols) x[[col]] <- 0
return(x)
}))
# Spalten sortieren: File + Tokens zuerst
first_cols <- c("File", "Tokens")
other_cols <- setdiff(names(final_results), first_cols)
final_results <- final_results[, c(first_cols, sort(other_cols))]
# Ergebnisse als CSV speichern
write.csv(final_results, file = output_file, row.names = FALSE)
cat("Analyse abgeschlossen. Ergebnisse gespeichert in:", output_file)
tab
combined_wide$prec_success
# Pakete laden
library(dplyr)
library(tidyr)
library(ggplot2)
library(readr)
# Dateien laden (Pfad ggf. anpassen)
success_data <- read.csv("~/Bookdown/EJIS mit Pragmatismus/success_kick_all_upos_counts2017.csv")
fail_data <- read.csv("~/Bookdown/EJIS mit Pragmatismus/failed_kick_all_upos_counts2017.csv")
# Beide um eine neue Spalte "Category" ergänzen
success_data$Category <- "Success"
fail_data$Category <- "Fail"
# Zusammenführen
combined_data <- bind_rows(success_data, fail_data)
combined_data$Freq = combined_data$Freq*100/combined_data$Tokens
combined_wide <- combined_data %>%
pivot_wider(names_from=Var2, values_from=Freq)
# combined_wide <- combined_wide[complete.cases(combined_wide),]
combined_wide[ , 5:21][is.na(combined_wide[ , 5:21])] <- 0
combined_wide$success <- ifelse (combined_wide$Category=="Success",1,NA)
combined_wide$success <- ifelse (combined_wide$Category=="Fail",0,combined_wide$success)
# Annahme: POS-Features (kontinuierlich)
# coefs wie oben:
coefs <- log(c(
Intercept = 0.8798575,
ADJ = 1.0230994,
ADP = 1.1182783,
ADV = 1.0135628,
AUX = 0.9101804,
CCONJ = 0.9828020,
DET = 1.0284228,
NOUN = 0.9342939,
NUM = 1.0632606,
PART = 0.8864184,
PRON = 1.0339938,
PROPN = 1.0366781,
PUNCT = 1.1063751,
SCONJ = 1.1282594,
SYM = 1.2247512,
VERB = 0.9194989
))
# Logit & Wahrscheinlichkeitsberechnung
combined_wide$logit <- coefs["Intercept"] +
coefs["ADJ"]*combined_wide$ADJ +
coefs["ADV"]*combined_wide$ADV +
coefs["AUX"]*combined_wide$AUX +
coefs["CCONJ"]*combined_wide$CCONJ +
coefs["DET"]*combined_wide$DET +
coefs["NOUN"]*combined_wide$NOUN +
coefs["NUM"]*combined_wide$NUM +
coefs["PART"]*combined_wide$PART +
coefs["PRON"]*combined_wide$PRON +
coefs["PROPN"]*combined_wide$PROPN +
coefs["PUNCT"]*combined_wide$PUNCT +
coefs["SCONJ"]*combined_wide$SCONJ +
coefs["SYM"]*combined_wide$SYM +
coefs["VERB"]*combined_wide$VERB
combined_wide$prob <- 1 / (1 + exp(-combined_wide$logit))
# Ausgabe
combined_wide$prob
combined_wide$prec_success <- ifelse (combined_wide$prob <=0.5,0,1)
table(combined_wide$prec_success, combined_wide$Category)
tab <- table(combined_wide$prec_success, combined_wide$Category)
# df[1,2] erste Zeile - zweite Spalte
# True Negativ (d) tab[1,1]
# False Negativ (c) tab[1,2]
# True Positiv (a) tab[2,2]
# False Positiv (b) tab[2,1]
sensitivity = tab[2,2]/ (tab[2,2] + tab[1,2])
sensitivity
specificity = tab[1,1]/ (tab[1,1] + tab[2,1])
specificity
precision = tab[2,2]/(tab[2,2]+tab[2,1])
precision = tab[2,2]/ (tab[2,2] + tab[2,1])
precision
F1_score= 2* (precision*sensitivity)/(precision+sensitivity)
F1_score
J = sensitivity + specificity-1
J
# Pakete laden
library(dplyr)
library(tidyr)
library(ggplot2)
library(readr)
# Dateien laden (Pfad ggf. anpassen)
success_data <- read.csv("~/Bookdown/EJIS mit Pragmatismus/success_kick_all_upos_counts.csv")
fail_data <- read.csv("~/Bookdown/EJIS mit Pragmatismus/failed_kick_all_upos_counts.csv")
# Beide um eine neue Spalte "Category" ergänzen
success_data$Category <- "Success"
fail_data$Category <- "Fail"
# Zusammenführen
combined_data <- bind_rows(success_data, fail_data)
combined_data$Freq = combined_data$Freq*100/combined_data$Tokens
combined_wide <- combined_data %>%
pivot_wider(names_from=Var2, values_from=Freq)
# combined_wide <- combined_wide[complete.cases(combined_wide),]
combined_wide[ , 5:21][is.na(combined_wide[ , 5:21])] <- 0
combined_wide$success <- ifelse (combined_wide$Category=="Success",1,NA)
combined_wide$success <- ifelse (combined_wide$Category=="Fail",0,combined_wide$success)
# Annahme: POS-Features (kontinuierlich)
# coefs wie oben:
coefs <- log(c(
Intercept = 0.8798575,
ADJ = 1.0230994,
ADP = 1.1182783,
ADV = 1.0135628,
AUX = 0.9101804,
CCONJ = 0.9828020,
DET = 1.0284228,
NOUN = 0.9342939,
NUM = 1.0632606,
PART = 0.8864184,
PRON = 1.0339938,
PROPN = 1.0366781,
PUNCT = 1.1063751,
SCONJ = 1.1282594,
SYM = 1.2247512,
VERB = 0.9194989
))
# Logit & Wahrscheinlichkeitsberechnung
combined_wide$logit <- coefs["Intercept"] +
coefs["ADJ"]*combined_wide$ADJ +
coefs["ADV"]*combined_wide$ADV +
coefs["AUX"]*combined_wide$AUX +
coefs["CCONJ"]*combined_wide$CCONJ +
coefs["DET"]*combined_wide$DET +
coefs["NOUN"]*combined_wide$NOUN +
coefs["NUM"]*combined_wide$NUM +
coefs["PART"]*combined_wide$PART +
coefs["PRON"]*combined_wide$PRON +
coefs["PROPN"]*combined_wide$PROPN +
coefs["PUNCT"]*combined_wide$PUNCT +
coefs["SCONJ"]*combined_wide$SCONJ +
coefs["SYM"]*combined_wide$SYM +
coefs["VERB"]*combined_wide$VERB
combined_wide$prob <- 1 / (1 + exp(-combined_wide$logit))
# Ausgabe
combined_wide$prob
combined_wide$prec_success <- ifelse (combined_wide$prob <=0.5,0,1)
table(combined_wide$prec_success, combined_wide$Category)
tab <- table(combined_wide$prec_success, combined_wide$Category)
# df[1,2] erste Zeile - zweite Spalte
# True Negativ (d) tab[1,1]
# False Negativ (c) tab[1,2]
# True Positiv (a) tab[2,2]
# False Positiv (b) tab[2,1]
sensitivity = tab[2,2]/ (tab[2,2] + tab[1,2])
sensitivity
specificity = tab[1,1]/ (tab[1,1] + tab[2,1])
specificity
precision = tab[2,2]/(tab[2,2]+tab[2,1])
precision = tab[2,2]/ (tab[2,2] + tab[2,1])
precision
F1_score= 2* (precision*sensitivity)/(precision+sensitivity)
F1_score
J = sensitivity + specificity-1
J
# Pakete laden
library(dplyr)
library(tidyr)
library(ggplot2)
library(readr)
# Dateien laden (Pfad ggf. anpassen)
success_data <- read.csv("~/Bookdown/EJIS mit Pragmatismus/success_kick_all_upos_counts2017.csv")
fail_data <- read.csv("~/Bookdown/EJIS mit Pragmatismus/failed_kick_all_upos_counts2017.csv")
# Beide um eine neue Spalte "Category" ergänzen
success_data$Category <- "Success"
fail_data$Category <- "Fail"
# Zusammenführen
combined_data <- bind_rows(success_data, fail_data)
combined_data$Freq = combined_data$Freq*100/combined_data$Tokens
combined_wide <- combined_data %>%
pivot_wider(names_from=Var2, values_from=Freq)
# combined_wide <- combined_wide[complete.cases(combined_wide),]
combined_wide[ , 5:21][is.na(combined_wide[ , 5:21])] <- 0
combined_wide$success <- ifelse (combined_wide$Category=="Success",1,NA)
combined_wide$success <- ifelse (combined_wide$Category=="Fail",0,combined_wide$success)
# Annahme: POS-Features (kontinuierlich)
# coefs wie oben:
coefs <- log(c(
Intercept = 0.8798575,
ADJ = 1.0230994,
ADP = 1.1182783,
ADV = 1.0135628,
AUX = 0.9101804,
CCONJ = 0.9828020,
DET = 1.0284228,
NOUN = 0.9342939,
NUM = 1.0632606,
PART = 0.8864184,
PRON = 1.0339938,
PROPN = 1.0366781,
PUNCT = 1.1063751,
SCONJ = 1.1282594,
SYM = 1.2247512,
VERB = 0.9194989
))
# Logit & Wahrscheinlichkeitsberechnung
combined_wide$logit <- coefs["Intercept"] +
coefs["ADJ"]*combined_wide$ADJ +
coefs["ADV"]*combined_wide$ADV +
coefs["AUX"]*combined_wide$AUX +
coefs["CCONJ"]*combined_wide$CCONJ +
coefs["DET"]*combined_wide$DET +
coefs["NOUN"]*combined_wide$NOUN +
coefs["NUM"]*combined_wide$NUM +
coefs["PART"]*combined_wide$PART +
coefs["PRON"]*combined_wide$PRON +
coefs["PROPN"]*combined_wide$PROPN +
coefs["PUNCT"]*combined_wide$PUNCT +
coefs["SCONJ"]*combined_wide$SCONJ +
coefs["SYM"]*combined_wide$SYM +
coefs["VERB"]*combined_wide$VERB
combined_wide$prob <- 1 / (1 + exp(-combined_wide$logit))
# Ausgabe
combined_wide$prob
combined_wide$prec_success <- ifelse (combined_wide$prob <=0.5,0,1)
table(combined_wide$prec_success, combined_wide$Category)
tab <- table(combined_wide$prec_success, combined_wide$Category)
# df[1,2] erste Zeile - zweite Spalte
# True Negativ (d) tab[1,1]
# False Negativ (c) tab[1,2]
# True Positiv (a) tab[2,2]
# False Positiv (b) tab[2,1]
sensitivity = tab[2,2]/ (tab[2,2] + tab[1,2])
sensitivity
specificity = tab[1,1]/ (tab[1,1] + tab[2,1])
specificity
precision = tab[2,2]/(tab[2,2]+tab[2,1])
precision = tab[2,2]/ (tab[2,2] + tab[2,1])
precision
F1_score= 2* (precision*sensitivity)/(precision+sensitivity)
F1_score
J = sensitivity + specificity-1
J
setwd ("~/Bookdown/EJIS mit Pragmatismus/data/kickstarter/pred_fail2018/")
input_dir <- "~/Bookdown/EJIS mit Pragmatismus/data/kickstarter/pred_fail2018/"  # Pfad anpassen
# Alle .txt-Dateien im Verzeichnis einlesen
text_files <- list.files(input_dir, pattern = "\\.txt$", full.names = TRUE)
# Rohdaten-File einlesen
failed <- read.csv("~/Bookdown/EJIS mit Pragmatismus/data/kickstarter/KickStarter_failed_projects_2018.csv")
# Überprüfen, ob die txt-Datein vorliegen, ansonsten Extraktion durchführen
if (nrow(data.frame(text_files)) == 0) {
# Sicherstellen, dass das "failed"-Dataframe existiert und die Spalten korrekt sind
# Spalten: story (String), risks (String), id (String oder numerisch)
for (i in 1:nrow(failed)) {
content <- paste(failed$story[i], failed$risks[i], sep = "\n\n")  # Trennung mit Leerzeile
filename <- paste0(failed$id[i], ".txt")
writeLines(content, con = filename)
}
}
setwd ("~/Bookdown/EJIS mit Pragmatismus/data/kickstarter/pred_success2018/")
# Alle .txt-Dateien im Verzeichnis einlesen
input_dir <- "~/Bookdown/EJIS mit Pragmatismus/data/kickstarter/pred_success2018/"  # Pfad anpassen
text_files <- list.files(input_dir, pattern = "\\.txt$", full.names = TRUE)
# Rohdaten-File einlesen
success <- read.csv("~/Bookdown/EJIS mit Pragmatismus/data/kickstarter/KickStarter_successful_projects_2018.csv")
# Überprüfen, ob die txt-Datein vorliegen, ansonsten Extraktion durchführen
if (nrow(data.frame(text_files)) == 0) {
# Sicherstellen, dass das "success"-Dataframe existiert und die Spalten korrekt sind
# Spalten: story (String), risks (String), id (String oder numerisch)
for (i in 1:nrow(success)) {
content <- paste(success$story[i], success$risks[i], sep = "\n\n")  # Trennung mit Leerzeile
filename <- paste0(success$id[i], ".txt")
writeLines(content, con = filename)
}
}
# Benötigte Pakete installieren und laden
# install.packages("udpipe", dependencies = TRUE)
# install.packages("plyr")
library(udpipe)
library(plyr)
# Englisches Modell für NLP laden (falls nötig herunterladen)
model <- udpipe_download_model(language = "english")
ud_model <- udpipe_load_model(model$file_model)
# Verzeichnis mit englischen Textdateien angeben
input_dir <- "~/Bookdown/EJIS mit Pragmatismus/data/kickstarter/pred_fail2018/"  # Pfad anpassen
output_file <- "failed_kick_all_upos_counts2018.csv"
# Alle .txt-Dateien im Verzeichnis einlesen
text_files <- list.files(input_dir, pattern = "\\.txt$", full.names = TRUE)
# Leere Ergebnisliste vorbereiten
all_results <- list()
# Analyse für jede Datei
for (file in text_files) {
# Datei einlesen
text <- paste(readLines(file, encoding = "UTF-8", warn = FALSE), collapse = " ")
text <- trimws(text)  # führende/trailende Leerzeichen entfernen
# Nur wenn Text nicht leer ist
if (nchar(text) > 0) {
# Annotation durchführen
annotation <- udpipe_annotate(ud_model, x = text)
annotation <- as.data.frame(annotation)
# UPOS-Zählung
upos_counts <- table(annotation$upos)
upos_df <- as.data.frame(t(upos_counts))  # Transponieren für eine Zeile pro Datei
upos_df$File <- basename(file)
upos_df$Tokens <- nrow(annotation)  # Gesamtanzahl der Token
all_results[[file]] <- upos_df
}
}
# Alle Ergebnisse zu einem DataFrame zusammenführen
final_results <- do.call(rbind.fill, lapply(all_results, function(x) {
# Fehlende Spalten auffüllen
missing_cols <- setdiff(unique(unlist(lapply(all_results, names))), names(x))
for (col in missing_cols) x[[col]] <- 0
return(x)
}))
# Spalten sortieren: File + Tokens zuerst
first_cols <- c("File", "Tokens")
other_cols <- setdiff(names(final_results), first_cols)
final_results <- final_results[, c(first_cols, sort(other_cols))]
# Ergebnisse als CSV speichern
write.csv(final_results, file = output_file, row.names = FALSE)
cat("Analyse abgeschlossen. Ergebnisse gespeichert in:", output_file)
# Benötigte Pakete installieren und laden
# install.packages("udpipe", dependencies = TRUE)
# install.packages("plyr")
library(udpipe)
library(plyr)
# Englisches Modell für NLP laden (falls nötig herunterladen)
model <- udpipe_download_model(language = "english")
ud_model <- udpipe_load_model(model$file_model)
# Verzeichnis mit englischen Textdateien angeben
input_dir <- "~/Bookdown/EJIS mit Pragmatismus/data/kickstarter/pred_success2018/"  # Pfad anpassen
output_file <- "success_kick_all_upos_counts2018.csv"
# Alle .txt-Dateien im Verzeichnis einlesen
text_files <- list.files(input_dir, pattern = "\\.txt$", full.names = TRUE)
# Leere Ergebnisliste vorbereiten
all_results <- list()
# Analyse für jede Datei
for (file in text_files) {
# Datei einlesen
text <- paste(readLines(file, encoding = "UTF-8", warn = FALSE), collapse = " ")
text <- trimws(text)  # führende/trailende Leerzeichen entfernen
# Nur wenn Text nicht leer ist
if (nchar(text) > 0) {
# Annotation durchführen
annotation <- udpipe_annotate(ud_model, x = text)
annotation <- as.data.frame(annotation)
# UPOS-Zählung
upos_counts <- table(annotation$upos)
upos_df <- as.data.frame(t(upos_counts))  # Transponieren für eine Zeile pro Datei
upos_df$File <- basename(file)
upos_df$Tokens <- nrow(annotation)  # Gesamtanzahl der Token
all_results[[file]] <- upos_df
}
}
# Alle Ergebnisse zu einem DataFrame zusammenführen
final_results <- do.call(rbind.fill, lapply(all_results, function(x) {
# Fehlende Spalten auffüllen
missing_cols <- setdiff(unique(unlist(lapply(all_results, names))), names(x))
for (col in missing_cols) x[[col]] <- 0
return(x)
}))
# Spalten sortieren: File + Tokens zuerst
first_cols <- c("File", "Tokens")
other_cols <- setdiff(names(final_results), first_cols)
final_results <- final_results[, c(first_cols, sort(other_cols))]
# Ergebnisse als CSV speichern
write.csv(final_results, file = output_file, row.names = FALSE)
cat("Analyse abgeschlossen. Ergebnisse gespeichert in:", output_file)
# Pakete laden
library(dplyr)
library(tidyr)
library(ggplot2)
library(readr)
# Dateien laden (Pfad ggf. anpassen)
success_data <- read.csv("~/Bookdown/EJIS mit Pragmatismus/success_kick_all_upos_counts2018.csv")
fail_data <- read.csv("~/Bookdown/EJIS mit Pragmatismus/failed_kick_all_upos_counts2018.csv")
# Beide um eine neue Spalte "Category" ergänzen
success_data$Category <- "Success"
fail_data$Category <- "Fail"
# Zusammenführen
combined_data <- bind_rows(success_data, fail_data)
combined_data$Freq = combined_data$Freq*100/combined_data$Tokens
combined_wide <- combined_data %>%
pivot_wider(names_from=Var2, values_from=Freq)
# combined_wide <- combined_wide[complete.cases(combined_wide),]
combined_wide[ , 5:21][is.na(combined_wide[ , 5:21])] <- 0
combined_wide$success <- ifelse (combined_wide$Category=="Success",1,NA)
combined_wide$success <- ifelse (combined_wide$Category=="Fail",0,combined_wide$success)
# Annahme: POS-Features (kontinuierlich)
# coefs wie oben:
coefs <- log(c(
Intercept = 0.8798575,
ADJ = 1.0230994,
ADP = 1.1182783,
ADV = 1.0135628,
AUX = 0.9101804,
CCONJ = 0.9828020,
DET = 1.0284228,
NOUN = 0.9342939,
NUM = 1.0632606,
PART = 0.8864184,
PRON = 1.0339938,
PROPN = 1.0366781,
PUNCT = 1.1063751,
SCONJ = 1.1282594,
SYM = 1.2247512,
VERB = 0.9194989
))
# Logit & Wahrscheinlichkeitsberechnung
combined_wide$logit <- coefs["Intercept"] +
coefs["ADJ"]*combined_wide$ADJ +
coefs["ADV"]*combined_wide$ADV +
coefs["AUX"]*combined_wide$AUX +
coefs["CCONJ"]*combined_wide$CCONJ +
coefs["DET"]*combined_wide$DET +
coefs["NOUN"]*combined_wide$NOUN +
coefs["NUM"]*combined_wide$NUM +
coefs["PART"]*combined_wide$PART +
coefs["PRON"]*combined_wide$PRON +
coefs["PROPN"]*combined_wide$PROPN +
coefs["PUNCT"]*combined_wide$PUNCT +
coefs["SCONJ"]*combined_wide$SCONJ +
coefs["SYM"]*combined_wide$SYM +
coefs["VERB"]*combined_wide$VERB
combined_wide$prob <- 1 / (1 + exp(-combined_wide$logit))
# Ausgabe
combined_wide$prob
combined_wide$prec_success <- ifelse (combined_wide$prob <=0.5,0,1)
table(combined_wide$prec_success, combined_wide$Category)
tab <- table(combined_wide$prec_success, combined_wide$Category)
# df[1,2] erste Zeile - zweite Spalte
# True Negativ (d) tab[1,1]
# False Negativ (c) tab[1,2]
# True Positiv (a) tab[2,2]
# False Positiv (b) tab[2,1]
sensitivity = tab[2,2]/ (tab[2,2] + tab[1,2])
sensitivity
specificity = tab[1,1]/ (tab[1,1] + tab[2,1])
specificity
precision = tab[2,2]/(tab[2,2]+tab[2,1])
precision = tab[2,2]/ (tab[2,2] + tab[2,1])
precision
F1_score= 2* (precision*sensitivity)/(precision+sensitivity)
F1_score
J = sensitivity + specificity-1
J
install.packages(c("admisc", "BH", "bit", "bit64", "bookdown", "broom", "bslib", "butcher", "caret", "chk", "class", "cli", "clock", "clue", "cluster", "collapse", "commonmark", "corrplot", "cpp11", "curl", "data.table", "dendextend", "DEoptimR", "doBy", "doRNG", "emmeans", "eRm", "evaluate", "fastICA", "fastmatch", "faux", "fontawesome", "forecast", "foreign", "fs", "future.apply", "gert", "ggplot2", "gower", "hardhat", "Hmisc", "httr2", "hunspell", "igraph", "insight", "ivreg", "jsonlite", "KernSmooth", "knitr", "later", "lattice", "lava", "lme4", "lpSolve", "lubridate", "marginaleffects", "MASS", "MatchIt", "Matrix", "MatrixModels", "mgcv", "mime", "minty", "mirt", "msm", "mvtnorm", "ndjson", "network", "nlme", "nloptr", "NLP", "nnet", "officer", "openssl", "parallelly", "pdftools", "pillar", "pkgbuild", "plm", "processx", "progressr", "promises", "ps", "psych", "purrr", "QCA", "qdapRegex", "qpdf", "quanteda", "quantmod", "quantreg", "R.oo", "R.utils", "R6", "Rcpp", "RcppArmadillo", "RcppParallel", "RCurl", "Rdpack", "readODS", "readxl", "recipes", "rgl", "rlang", "rmarkdown", "Rmpfr", "rpart", "semTools", "sessioninfo", "sfsmisc", "shiny", "SimDesign", "slam", "spatial", "statnet.common", "stringi", "survival", "systemfonts", "testthat", "textshaping", "tinytex", "tm", "tzdb", "usethis", "V8", "vegan", "VGAM", "waldo", "xfun", "XML", "xml2", "zip", "zoo"))
usethis::edit_r_environ()
restart
lavaan.shiny()
library(shiny)
lavaan.shiny()
lavaangui
install.packages("lavaangui")
library(lavaangui)
lavaangui()
cfa()
library(Statsomat)
install.packages(Statsomat)
install.packages("Statsomat")
library(Statsomat)
cfa()
# Wenn wir die Sales für 2019
#McKenny, A. F., Short, J. C., Ketchen Jr, D. J., Payne, G. T., & Moss, T. W. (2018). Strategic entrepreneurial orientation: Configurations, performance, and the effects of industry and time. Strategic Entrepreneurship Journal, 12(4), 504-521. https://doi.org/10.1002/sej.1291
#Kindermann, B., Beutel, S., de Lomana, G. G., Strese, S., Bendig, D., & Brettel, M. (2021). Digital orientation: Conceptualization and operationalization of a new strategic orientation. European Management Journal, 39(5), 645-657. https://doi.org/10.1016/j.emj.2020.10.009
setwd("~/Windows Exchange Link/2023 JIS_JMS - Posture ODI_BMI/2023 Strategic Posture/2023 EJIS Know Your Growth Focus/Reviews 10-2023")
setwd("~/")
